.. _header-n0:

NLP 2
=======

.. _header-n3:

1.中文分词
----------

.. _header-n4:

1.1 中文分词方法
~~~~~~~~~~~~~~~~

.. _header-n5:

1.1.1 规则分词
^^^^^^^^^^^^^^

   基于规则的分词是一种机械的分词方法，主要通过维护词典，在切分语句时，将语句的每个字符串与词汇表中的词逐一进行匹配，找到则切分，否则不予切分；

.. _header-n8:

1.1.1.1 正向最大匹配法
''''''''''''''''''''''

.. code:: python

   class MM(object):
   	def __init__(self):
   		self.window_size = 3

   	def cut(self, text):
   		result = []
   		index = 0
   		text_length = len(text)
   		dic = ["研究", "研究生", "生命", "命", "的", "起源"]
   		while text_length > index:
   			for size in range(self.window_size + index, index, -1):
   				piece = text[index:size]
   				if piece in dic:
   					index = size - 1
   					break
   			index = index + 1
   			result.append(piece + "----")
   		print(result)

   if __name__ == "__main__":
   	text = "研究生命的起源"
   	tokenizer = MM()
   	print(tokenizer.cut(text))

.. _header-n10:

1.1.1.2 正向最大匹配法
''''''''''''''''''''''

.. _header-n11:

1.1.1.3 双向最大匹配法
''''''''''''''''''''''

.. _header-n12:

1.1.2 统计分词
^^^^^^^^^^^^^^

.. _header-n14:

1.1.3 混合分词
^^^^^^^^^^^^^^

.. _header-n16:

1.2 中文分词工具
~~~~~~~~~~~~~~~~

   -  jieba

   -  精确模式

      -  试图将句子最精确地切开，适合文本分析；

   -  全模式

      -  把句子中的所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义；

   -  搜索引擎模式

      -  在精确模式的基础上，对长词进行再次切分，提高召回率，适合用于搜索引擎分词；

安装:

.. code:: shell

   pip install jieba

三种分词模式：

.. code:: python

   import jieba

   sent = "中文分词是文本处理不可或缺的一步！"

   # 精确模式
   seg_list = jieba.cut(sent, cut_all = False)

   # 全模式
   seg_list = jieba.cut(sent, cut_all = True)

   # 搜索引擎模式
   seg_list = jieba.cut_for_search(sent)

加载自定义词典

.. code:: python

   jieba.load_userdict("./data/user_dict.utf8")

示例：高频词提取

.. code:: python

   # 读取数据
   def get_content(path):
   	with open(path, "r", encoding = "gbk", errors = "ignore") as file:
   		content = ""
   		for line in file:
   			line = line.strip()
   			content += line
   		return content

   # 定义高频词统计函数
   def get_TF(words, topK = 10):
   	tf_dic = {}
   	for w in words:
   		tf_dic[w] = tf_dic.get(w, 0) + 1
   		return sorted(tf_dic.items(), key = lambda x: x[1], reverse = True)[:topK]

   # 调用停用词典，过滤停用词
   def stop_words(path):
   	with open(path) as file:
   		return [line.strip() for line in file]

   # 加载自定义领域词典提高分词效果


   def main():
   	import glob
   	import random
   	import jieba
   	files = glob.glob("./data/news/C000013/*.txt")
   	corpus = [get_content(x) for x in files]
   	sample_inx = random.randint(0, len(corpus))
   	split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words("./data/stop_words.utf8")]
   	print("样本之一：" + corpus[sample_inx])
   	print("样本分词效果：" + "/ ".join(split_words))
   	print("样本的topK(10)词：" + str(get_TF(split_words)))

.. _header-n46:

2.词性标注，命名实体识别
------------------------

.. _header-n48:

2.1 词性标注
~~~~~~~~~~~~

   -  词性标注是在给定句子中判断每个词的语法范畴，确定其词性并加以标注的过程；

   -  词性标注最简单的方法是从预料库中统计每个词对应的高频词性，将其作为默认的词性；

.. _header-n56:

2.1.1 词性标注规范
^^^^^^^^^^^^^^^^^^

-  北大词性标注集

-  宾州词性标注集

.. _header-n63:

2.1.2 jieba 分词中的词性标注
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  类似分词流程，jieba
   的词性标注同样是结合规则和统计的方式，具体为在词性标注的过程中，词典匹配和
   HMM 共同作用；

-  词性标注流程：

   -  

.. _header-n73:

2.2 命名实体识别
~~~~~~~~~~~~~~~~

.. _header-n75:

3.关键词提取
------------

.. _header-n78:

4.句法分析
----------

.. _header-n80:

5.自然语言处理与词向量
----------------------

   自然语言处理主要研究使用计算机来处理、理解以及运用人类语言的各种理论和方法，属于人工智能的一个重要研究方向；

.. _header-n84:

5.1 词汇表征
~~~~~~~~~~~~

.. _header-n85:

5.2 词向量与语言模型
~~~~~~~~~~~~~~~~~~~~

.. _header-n87:

6.word2vec 词向量
-----------------

从深度学习的角度看，假设将 NLP
的语言模型看作是一个监督学习问题：给定上下文词 :math:`X`\ ，输出中间词
:math:`Y`\ ；或者给定中间词 :math:`X`\ ，输出上下文词
:math:`Y`\ 。基于输入 :math:`X` 和输出 :math:`Y`
之间的映射便是语言模型。这样的一个语言模型的目的便是检查 :math:`X` 和
:math:`Y` 放在一起是否符合自然语言规则，更通俗一点就是 :math:`X` 和
:math:`Y` 放在一起是不是人话。

所以，基于监督学习的思想，word2vec
便是一种基于神经网络训练的自然语言模型。word2vec 是谷歌于 2013
年提出的一种 NLP
工具，其特点就是将词汇进行向量化，这样就可以定量的分析和挖掘词汇之间的联系。因而
word2vec
也是词嵌入表征的一种，只不过这种向量表征需要经过神经网络训练得到。

word2vec 训练神经网路得到的一个关于输入 :math:`X` 和输出 :math:`Y`
之间的语言模型，关注的重点并不是说要把这个模型训练的有多好，而是要获取训练好的神经网络权重，这个权重就是我们要拿来对输入词汇
:math:`X` 的向量化表示。一旦拿到了训练预料所有词汇的词向量，接下来开展
NLP 分析工作就相对容易一些。

.. _header-n92:

7.词向量的训练
--------------

.. _header-n94:

自然语言处理(NLP)内容架构
~~~~~~~~~~~~~~~~~~~~~~~~~

-  **句法语义分析**

   -  对给定的句子进行：

   -  分词

   -  词性标记

   -  命名实体识别和链接

   -  句法分析

   -  语义角色识别

   -  多义词消歧

-  **信息抽取**

   -  从给定文本中抽取重要的信息，比如时间，地点，人物，事件，原因，结果，数字，货币，日期，专有名词等；

   -  目的：了解谁在什么时候，什么地方，什么原因，对谁，做了什么事，有什么结果。

   -  技术：

   -  实体识别

   -  时间抽取

   -  因果关系抽取

   -  ...

-  **文本挖掘**

   -  挖掘步骤：

   -  文本收集

   -  文本分析

   -  特征修剪

   -  挖掘任务：

   -  文本聚类

   -  文本分类

   -  信息抽取

   -  摘要抽取

   -  情感分析

   -  对挖掘的信息和知识可视化

   -  交互式表达界面

-  **机器翻译**

   -  把输入的源语言文本通过自动翻译获得另外一种语言的文本

   -  根据输入媒介不同，可分为：

   -  文本翻译

   -  语音翻译

   -  手语翻译

   -  图形翻译

   -  技术：

   -  统计方法

   -  神经网络方法(编码-解码)

-  **信息检索**

   -  对大规模的文档进行索引。

   -  可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。

-  **问答系统**

   -  对一个自然语言表达的问题，由问答系统给出一个精准的答案。

   -  需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。

-  **对话系统**

   -  系统通过一系列的对话，跟用户进行聊天，回答，完成某一项任务。

   -  涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。

.. _header-n202:

自然语言处理(NLP)模型架构
~~~~~~~~~~~~~~~~~~~~~~~~~

-  隐马尔可夫模型

   -  问题1（似然度问题）：给一个HMM λ=（A,B）
      和一个观察序列O，确定观察序列的似然度问题 P(O|λ)
      。（向前算法解决）

   -  问题2（解码问题）：给定一个观察序列O和一个HMM
      λ=（A,B），找出最好的隐藏状态序列Q。（维特比算法解决）

   -  问题3（学习问题）：给定一个观察序列O和一个HMM中的状态集合，自动学习HMM的参数A和B。（向前向后算法解决）

.. _header-n213:

自然语言处理(NLP)技术架构
~~~~~~~~~~~~~~~~~~~~~~~~~

-  中文分词

   -  分词方法

   -  规则分词

   -  正向最大匹配法

   -  逆向最大匹配法

   -  双向最大匹配法

   -  统计分词

   -  混合分词(规则+统计)

   -  常用分词库

   -  StanfordNLP

   -  哈工大语言云

   -  庖丁解牛分词

   -  盘古分词 (ICTCLAS, 中科院汉语词法分析系统)

   -  IKAnalyzer（Luence项目下，基于java）

   -  FudanNLP（复旦大学）

   -  中文分词工具

   -  ``Ansj``

   -  盘古分词

   -  ``jieba``

-  词性标注

-  命名实体识别

-  关键词提取

-  句法分析

-  文本向量化

-  情感分析

-  CRF：在CRF for Chinese
   NER这个任务中，提取的特征大多是该词是否为中国人名姓氏用字，该词是否为中国人名名字用字之类的，True
   or
   false的特征。所以一个可靠的百家姓的表就十分重要啦~在国内学者做的诸多实验中，效果最好的人名可以F1测度达到90%，最差的机构名达到85%。

-  字典法：在NER中就是把每个字都当开头的字放到trie-tree中查一遍，查到了就是NE。中文的trie-tree需要进行哈希，因为中文字符太多了，不像英文就26个。

-  对六类不同的命名实体采取不一样的手段进行处理，例如对于人名，进行字级别的条件概率计算。
   中文：哈工大（语言云）上海交大 英文：stanfordNER等
