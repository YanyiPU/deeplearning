.. _header-n0:

参数初始化
==========

.. _header-n3:

1.参数初始化
------------

   -  在神经网络的学习中，权重 :math:`W`
      的的初始值特别重要。设定什么样的权重初始值，经常关系到神经网络的学习能否成功；

   -  不能将权重初始值全部设为0：

   -  因为在误差反向传播中，所有权重都会进行相同的更新；

      -  比如：在2层神经网络中，假设第1层和第2层的权重为0，这样一来，正向传播时，因为输入层的权重是0，所有第2层的权重神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行姓童的更新，因此，权重被更新为相同的值，并拥有了对称的值，这使得神经网络拥有许多不同的权重的意义就丧失了。

   -  为了防止“权重均一化”，必须随机生成初始值；

   -  梯度消失(gradient vanishing)

   -  当使用sigmoid函数作为激活函数时，激活层的激活值呈偏向0和1的分布，随着输出不断靠近0或1，它导数的值逐渐接近0，因此，偏向0和1的数据分布会造成反向传播中梯度的值不断减小，最后消失。层次加深的深度学习中，梯度消失的问题更加严重；

   -  表现力受限

   -  如果有多个神经元都输出几乎相同的值，那他们就没有存在的意义了。比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表示基本相同的事情。因此，激活值在分布上有所偏向；

   -  各层的激活函值的分布都要求有适当地广度。

   -  因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习，反之，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习无法顺利进行；

.. _header-n36:

Xavier初始值：
~~~~~~~~~~~~~~

   在Xavier
   Glorot等人的论文中，推荐了权重初始值，俗称“Xavier初始值”。在一般的深度学习框架中，Xavier初始值已经被作为标准使用；

Xavier的论文中，为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。推导出的结论是：

**:math:`如果前一层的节点数为 n，则初始值使用标准差为 \frac{1}{\sqrt{n}} 的分布；`**

.. _header-n41:

He初始值
~~~~~~~~

   Xavier初始值是以激活函数是线性函数为前提推导出来的，因为sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值；
   当激活函数使用ReLU函数时，一般推荐ReLU专用的初始值，Kaiming
   He等人推荐了一种初始值，俗称“He初始值”；

:math:`当如果前一层的节点数为 n，则初始值使用标准差为\sqrt{\frac{2}{n}}的高斯分布；`

**结论：**

-  当激活函数使用ReLU时，权重初始值使用\ **He初始值**\ ；

-  当激活函数为sigmoid或tanh等S型函数时，初始值使用\ **Xavier初始值**\ ；

.. _header-n52:

2.Batch Normalization
---------------------

   -  设定合适的权重初始值，各层的激活值分布就会有适当地广度，从而可以顺利地进行学习；

   -  为了使各层拥有适当的广度，Batch Normalization
      方法“强制性”地调整激活值的分布；

.. _header-n59:

2.1 Batch Normalization原理
~~~~~~~~~~~~~~~~~~~~~~~~~~~

   -  Batch Normalization
      的思路是调整各层的激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行的正规化层，即Batch
      Normalization层。

Batch
Normalization，顾名思义，以进行学习时的mini-batch为单位，按mini-batch进行正规化。具体来说，就是对mini-batch数据进行数据分布的均值为0，方差为1的正规化，数学表示如下：

:math:`\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i`

:math:`\sigma_{B}^{2} \leftarrow \frac{1}{m}\sum_{i=1}^{m}(x_i - \mu_B)^2`

:math:`\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_{B}^{2} + \epsilon}}`

这里对 mini-batch 的 m 个输入数据的集合
:math:`B=\{x_1, x_2, \ldots, x_m\}` 求均值 :math:`\mu_B`\ 和方差
:math:`\sigma_B^{2}`\ 。然后对输入数据进行均值为 0，方差为 1
的正规化。其中 :math:`\epsilon` 取一个较小的值
:math:`10e-7`\ 。即将mini-batch的输入数据
:math:`\{x_1, x_2, \ldots, x_m\}` 变换为均值为0，方差为1的数据
:math:`\{\hat{x_1}, \hat{x_2}, \ldots, \hat{x_m}\}`\ 。通过将这个处理插入到激活函数的前面或后面，可以减小数据分布的偏向。

接着Batch
Normalization层会对正规化后的数据进行缩放和平移的变换，数学表示如下：

:math:`y_i \leftarrow \gamma \hat{x_i} + \beta`

其中：

-  :math:`\gamma` 和 :math:`\beta`\ ： 是参数，初始值一般设为
   :math:`\gamma=1`, :math:`\beta=0`\ ，然后通过学习整合到合适的值；

.. _header-n75:

2.2 Batch Normalization优点
~~~~~~~~~~~~~~~~~~~~~~~~~~~

-  可以使学习快速进行(可以增大学习率)；

-  不那么依赖初始值(对初始值不敏感)；

-  抑制过拟合(降低Dropout等的必要性)；

.. _header-n84:

3.超参数的调优
--------------

   -  神经网络中的超参数是指，各层的神经元数量，batch大小，参数更新时的学习率，权值衰减参数(正则化参数)等；

   -  不能使用测试数据评估超参数的性能

   -  调整超参数时，必须使用超参数专用的确认数据，用于调整超参数的数据一般称为验证数据(validation
      data)；

   -  模型训练数据的使用：

   -  训练数据用于参数(权重和偏置)的学习；

   -  验证数据用于超参数的性能评估；

   -  测试数据确认泛化能力，要在最后使用(比较理想的是只用一次)；
