
NLP
==============

1.NLP 知识框架
------------------------

1.1 什么是 NLP?
~~~~~~~~~~~~~~~~~~~~~~~~

   - **(1) NLP 的概念**

      - 1.NLP(Natural Language Processing, 自然语言处理)是计算机领域以及人工智能领域的一个重要的研究方向,
         它研究用计算机来处理、理解以及运用人类语言,达到人与计算机之间的有效通讯.从建模的角度看,为了方便计算机处理.

      - 2.NLP 研究表示语言能力、语言应用的模型,通过建立计算机框架来实现这样的语言模型,并且不断完善这样的语言模型,
         还需要根据该语言模型来设计各种使用的系统,并且探讨这些实用技术的评测技术.

      - 3.从自然语言的角度出发,NLP 结构如下: 

         - **NLP**

            - **自然语言理解**

               - **音系学**: 指代语言中发音的系统化组织
               - **词态学**: 研究单词构成以及相互之间的关系
               - **句法学**: 给定文本的那本分是语法正确的
               - **语义学**: 给定文本的含义是什么
               - **语用学**: 文本的目的是什么

            - **自然语言生成** (Natural Language Generation, NLG): 从结构化数据中以读取的方式自动生成文本.该过程主要
               包含三个阶段: 文本规划(完成机构化数据中的基础内容规划)、语句规划(从结构化数据中组合语句来表达信息流)、
               实现(产生语法通顺的语句来表达文本)

               - **自然语言文本**

   - **(2) NLP 的研究任务**

      - **机器翻译**: 计算机具备将一种语言翻译成另一种语言的能力

      - **情感分析**: 计算机能够判断用户评论是否积极

      - **智能问答**: 计算机能够正确回答输入的问题

      - **文摘生成**: 计算机能够准确归纳、总结并产生文本摘要

      - **文本分类**: 计算机能够采集各种文章,进行主题分析,从而进行自动分类

      - **舆论分析**: 计算机能够判断目前舆论的导向

      - **知识图谱**: 知识点相互连接而成的语义网路

1.2 NLP 相关知识的构成
~~~~~~~~~~~~~~~~~~~~~~~~

   - **(1) 基本术语**

      - **分词(segment)**

         - 分词常用的方法是基于字典的最长串匹配,但是歧义分词很难

      - **词性标注(part-of-speech tagging)**

         - 词性一般是指动词(noun)、名词(verb)、形容词(adjective)等

         - 标注的目的是表征词的一种隐藏状态,隐藏状态构成的转移就构成了状态转义序列

      - **命名实体识别(NER, Named Entity Recognition)**

         - 命名实体是指从文本中识别具有特定类别的实体(通常是名词)

      - **句法分析(syntax parsing)**

         - 句法分析是一种基于规则的专家系统.句法分析的目的是解析句子中各个成分的依赖关系.
            所以,往往最终生成的结果是一棵句法分析树.句法分析可以解决传统词袋模型不考虑上下文的问题

      - **指代消解(anaphora resolution)**

         - 中文中带刺出现的频率很高,它的作用是用来表征前文出现过的人名、地名等

      - **情感识别(emotion recognition)**

         - 情感识别本质上是分类问题.通常可以基于词袋模型+分类器,或者现在流行的词向量模型+RNN

      - **纠错(correction)**

         - 基于 N-Gram 进行纠错、通过字典树纠错、有限状态机纠错

      - **问答系统(QA system)**

         - 问答系统往往需要语言识别、合成、自然语言理解、知识图谱等多项技术的配合才会实现得比较好

   - **(2) 知识结构**

      - 句法语义分析

      - 关键词抽取

      - 文本挖掘

      - 机器翻译

      - 信息检索

      - 问答系统

      - 对话系统

1.3 NLP 的三个层面
~~~~~~~~~~~~~~~~~~~~~~~~

   - **(1) 词法分析**

      - ``分词``
      - ``词性标注``

   - **(2) 句法分析**

      - 短语结构句法体系
      - 依存结构句法体系
      - 深层文法句法分析

   - **(3) 语义分析**

      - 语义角色标注(semantic role labeling)

1.4 NLP 常用语料库
~~~~~~~~~~~~~~~~~~~~~~~~

   - **(1) 中文**

      - `中文维基百科 <https://dumps.wikimedia.org/zhwiki/>`_ 

      - `搜狗新闻语料库 <http://download.labs.sogou.com/resource/ca.php>`_ 

      - `IMDB 情感分析语料库 <https://www.kaggle.com/tmdb/tmdb-moive-metadata>`_ 

      - 豆瓣读书

      - 邮件相关

   - **(2) 英文**

1.5 NLP 实现工具
~~~~~~~~~~~~~~~~~~~~~~~~

   - numpy

   - 正则表达式


2.中文分词(segment)
--------------------------------------

   在语言理解中,词是最小的能够独立活动的有意义的语言成分.将词确定下来是理解自然语言的第一步,只有跨越了这一步,中文才能像英文那样过度到
   短语划分、概念抽取以及主题分析,以致自然语言理解,最终达到智能计算的最高境界.

   “词”的概念一直是汉语言语言学界纠缠不清而又绕不开的问题.主要难点在于汉语结构与印欧体系语种差异甚大,对词的构成边界方面很难进行界定.
   比如,在英语中,单词本身就是“词”的表达,一篇英文文章就是“单词”加分隔符(空格)来表示的,

   而在汉语中,词以字为基本单位的,但是一篇文章的语义表达却仍然是以词来划分的.因此,在处理中文文本时,需要进行分词处理,将句子转化为词的表示.
   这个切词处理过程就是 **中文分词**,它通过计算机自动识别出句子的词,在词间加入边界标记符,分隔出各个词汇.整个过程看似简单,然而实践起来却很复杂,
   主要的困难在于 **分词歧义**.其他影响分词的因素是: **未登录词**、**分词粒度粗细**.

   自中文自动分词被提出以来,历经将近 30 年的探索,提出了很多方法,可主要归纳为：

      - 规则分词

      - 统计分词

      - 混合分词(规则+统计)

2.1 规则分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   基于规则的分词是一种机械的分词方法,主要通过维护词典,在切分语句时,将语句的每个字符串与词汇表中的词逐一进行匹配,
   找到则切分,否则不予切分.

   按照匹配切分的方式,主要有 **正向最大匹配法**、**逆向最大匹配法** 以及 **双向最大匹配法**.

2.1.1 正向最大匹配法
^^^^^^^^^^^^^^^^^^^^^


   .. code-block:: python

      class MM(object):
         def __init__(self):
            self.window_size = 3

         def cut(self, text):
            result = []
            index = 0
            text_length = len(text)
            dic = ["研究", "研究生", "生命", "命", "的", "起源"]
            while text_length > index:
                  for size in range(self.window_size + index, index, -1):
                     piece = text[index:size]
                     if piece in dic:
                        index = size - 1
                        break
                  index = index + 1
                  result.append(piece + "----")
            print(result)

      if __name__ == "__main__":
         text = "研究生命的起源"
         tokenizer = MM()
         print(tokenizer.cut(text))


2.1.2 反向最大匹配法
^^^^^^^^^^^^^^^^^^^^^


2.1.3 双向最大匹配法
^^^^^^^^^^^^^^^^^^^^^


2.2 统计分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   统计分词的主要思想是：把每个词看做是由词的最小单位的各个字组成的,如果相连的字在不同的文本中出现的次数越多,就证明这相连的字很可能就是一个词.
   因此我们就可以利用 **字与字相邻出现的频率** 来反应 **成词的可靠度**,统计语料中相邻共现的各个字的组合的频度,当组合频度高于某一个临界值时,便可以认为此字组
   成会构成一个词语.

   基于统计的分词,一般要做如下两步操作：

2.3 混合分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



3.词性标注
------------------------

3.1 词性标注介绍
~~~~~~~~~~~~~~~~~~

   词性是词汇基本的语法属性,通常称为 **词类**.词性标注是在给定句子中判断每个词的语法范畴,确定其词性并加以标注的过程.

   在中文中,一个词的词性很多时候都不是固定的,一般表现为同音同形的词在不同场景下,其表示的语法截然不同,这就为词性标注带来很大的困难；
   但是另外一方面,从整体上看,大多数词语,尤其是实词,一般只有一到两个词性,且其中一个词性的使用频次远远大于另一个,即使每次将高频词性
   选择进行标注,也能实现 80% 以上的准确率.如此,若我们对常用的词性能够进行很好地识别,那么就能够覆盖绝大多数场景,满足基本的准确度要求.

   词性标注最简单的方法是从预料库中统计每个词对应的高频词性,将其作为默认的词性.但这样显然还有提升空间.

   目前较为主流的方法是如同分词一样,将句子的词性标注作为一个序列标注问题来解决,那么分词中常用的手段,如隐含马尔科夫模型,条件随机场模型等
   皆可在词性标注任务中使用.

3.2 词性标注规范
~~~~~~~~~~~~~~~~~~

   词性标注需要有一定的标注规范,如将词分为名词、形容词、动词, 然后用 ``n``、``adj``、``v`` 等来进行表示.

   中文领域中尚无统一的标注标准,较为主流的是：

      -  北大词性标注集

      -  宾州词性标注集

3.2.1 北大词性标准集
^^^^^^^^^^^^^^^^^^^^

======== ============ ==================================================
标记      词性          说明
======== ============ ==================================================
``ag``   形语素         形容词性语素. 形容词代码为 a, 语素代码 g 前面置以 a
``a``    形容词         取英语形容词 adjective 的第 1 个字母
``ad``   副形词         直接作状语的形容词. 形容词代码 a 和副词代码 d 并在一起
``an``   名形词         具有名词功能的形容词. 形容词代码 a 和名词代码 n 并在一起
``b``    区别词         取汉字“别”的声母
``c``    连词           取英语连词 conjunction 的第 1 个字母
``dg``   副语素         副词性语素.副词代码为 d, 语素代码 ｇ 前面置以 d
``d``    副词           取 adverb 的第 2 个字母, 因其第 1 个字母已用于形容词
``e``    叹词           取英语叹词 exclamation 的第 1 个字母
``f``    方位词         取汉字“方”的声母
``g``    语素           绝大多数语素都能作为合成词的“词根”, 取汉字“根”的声母
``h``    前接成分        取英语 head 的第 1 个字母
``i``    成语           取英语成语 idiom 的第 1 个字母
``j``    简称略语        取汉字“简”的声母
``k``    后接成分
``l``    习用语         习用语尚未成为成语, 有点“临时性”, 取“临”的声母
``m``    数词           取英语 numeral 的第 3 个字母, n ,u 已有他用
``ng``   名语素         名词性语素.名词代码为 n, 语素代码 g 前面置以 n
``n``    名词           取英语名词 noun 的第 1 个字母
``nr``   人名           名词代码 n 和“人(ren)”的声母并在一起
``ns``   地名           名词代码 n 和处所词代码 s 并在一起
``nt``   机构团体       “团”的声母为 t, 名词代码 n 和 t 并在一起
``nz``   其他专名       “专”的声母的第 1 个字母为 z, 名词代码 n 和 z 并在一起
``o``    拟声词         取英语拟声词 onomatopoeia 的第 1 个字母
``p``    介词           取英语介词 prepositional 的第 1 个字母
``q``    量词           取英语 quantity 的第 1 个字母
``r``    代词           取英语代词 pronoun 的第 2 个字母, 因 p 已用于介词
``s``    处所词         取英语 space 的第 1 个字母
``Tg``   时语素         时间词性语素. 时间词代码为 t, 在语素的代码 g 前面置以 t
``t``    时间词         取英语 time 的第1个字母
``u``    助词           取英语助词 auxiliary 的第 2 个字母, 因 a 已用于形容词
``vg``   动语素         动词性语素.动词代码为 v. 在语素的代码 g 前面置以 v
``v``    动词           取英语动词 verb 的第一个字母
``vd``   副动词         直接作状语的动词. 动词和副词的代码并在一起
``vn``   名动词         指具有名词功能的动词. 动词和名词的代码并在一起
``w``    标点符号   
``x``    非语素字       非语素字只是一个符号, 字母 x 通常用于代表未知数、符号
``y``    语气词         取汉字“语”的声母
``z``    状态词         取汉字“状”的声母的前一个字母
======== ============ ==================================================


3.3 jieba 分词中的词性标注
~~~~~~~~~~~~~~~~~~~~~~~~~~~

   类似分词流程, ``jieba`` 的词性标注同样是结合规则和统计的方式,具体为在词性标注的过程中,词典匹配和 HMM 共同作用。词性标注流程如下：

      - 1.首先,基于正则表达式进行汉字判断, 正则表达式如下: 
      
         .. code-block:: python

            import re
            
            re_han_internal = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&\._]+)")

      - 2.若符合上面的正则表达式，则判定为汉字，然后基于前缀词典构建有向无环图，再基于有向无环图计算最大概率路径，同时在前缀词典中
        找出它所分出的词性，若在词典中未找到，则赋予词性为  ``x`` (代表未知)。当然，若在这个过程中，设置使用 HMM，且待标注词为未登录词，
        则会通过 HMM 方式进行词性标注。
      
      - 3.若不符合上面的正则表达式，那么将继续通过正则表达式进行类型判断，分别赋予 ``x``、``m`` (数词)、``eng`` (英文)


   使用 ``Jieba`` 分词进行词性标注示例:

      .. code-block:: python

         import jieba.posseg as psg

         sent = "中文分词是文本处理不可或缺的一步!"
         seg_list = psg.cut(sent)
         seg_list_hmm = psg.cut(sent, HMM = True)
         print(" ".join([f"{w}/{t}" for w, t in seg_list]))
         print(" ".join([f"{w}/{t}" for w, t in seg_list_hmm]))

   .. note:: 

      ``Jieba`` 分词支持自定义词典，其中的词频和词性可以省略。然而需要注意的是，若在词典中省略词性，那采用 ``Jieba`` 分词进行词性标注后，
      最终切分词的词性将变成 ``x``，这在如语法分析或词性统计等场景下会对结果有一定的影响。因此，在使用 ``Jieba`` 分词设置自定义词典时，
      尽量在词典中补充完整的信息。

4.命名实体识别
----------------------

4.1 命名实体识别介绍
~~~~~~~~~~~~~~~~~~~~~~~

与自动分词、词性标注一样，命名实体识别也是自然语言处理的一个基础任务，是信息抽取、信息检索、机器翻译、
问答系统等多种自然语言处理技术必不可少的组成部分。

命名实体识别的目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，
通常不可能在词典中穷尽列出，且其构成方法具有各自的规律性，因此，通常把对这些词的识别在词汇形态
处理(如汉语切分)任务中独立处理，称为命名实体识别(Named Entities Recognition, NER)。

命名实体识别研究的命名实体一般分为：

   - 3 大类：

      - 实体类

      - 时间类

      - 数字类

   - 7 小类:

      - 人名

      - 地名

      - 组织机构名

      - 时间

      - 日期

      - 货币

      - 百分比

由于数量、时间、日期、货币等

4.2 基于条件随机场的命名实体识别
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




4.3 示例
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

4.3.1 日期识别
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



4.3.2 地名识别
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^






5.关键词提取算法
----------------------

5.1 关键词提取介绍
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

关键词是代表文章重要内容的一组词。对文本聚类、分类、自动摘要等起重要作用。此外，它还能使人们便捷地浏览和获取信息。
类似于其他的机器学习方法，关键词提取算法一般也可以分为有监督和无监督两类:

   - 有监督的关键词提取方法主要是通过分类的方式进行，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中每个词的匹配程度，
     以类似打标签的方式，达到关键词提取的效果。

      - 有监督的方法能够获取到较高的精度，但缺点是需要大批量的标注数据，人工成本过高

      - 另外，现在每天的信息增加过多，会有大量的新信息出现，一个固定的词表有时很难将新信息的内容表达出来，
        但是要人工维护这个受控的词表却要很高的人力成本，这也是使用有监督方法来进行关键词提取的一个比较大的缺陷

   - 无监督的方法对数据的要求比较低，既不需要一张人工生成、维护的词表，也不需要人工标准语料辅助进行训练。
     因此，这类算法在关键词提取领域的应用更受到大家的青睐。

      - TF-IDF 算法
      - TextRank 算法
      - 主题模型算法

         - LSA
         - LSI
         - LDA

5.2 关键词提取算法--TF-IDF 算法
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TF-IDF 算法(Term Frequency-Inverse Document Frequency，词频-逆文档频次算法)，是一种基于统计的计算方法，
常用于评估一个文档集中一个词对某份文档的重要程度。这种作用显然很符合关键字抽取的需求，一个词对文档越重要，那就越可能
是文档对的关键词，常将 TF-IDF 算法应用于关键词提取中。

从算法的名称就可以看出，TF-IDF 算法由两部分组成：

   - **TF 算法**

      - TF 算法是统计一个词在一篇文档中出现的频次，其基本思想是，一个词在文档中出现的次数越多，则其对文档的表达能力就越强

   - **IDF 算法**

      - IDF 算法则是统计一个词在文档集的多少个文档中出现，其基本思想是，如果一个词在越少的文档中出现，则其对文档的区分能力也就越强

TF 算法和 IDF 算法也能单独使用，在最早的时候就是如此，但在使用过程中，学者们发现这两种算法都有不足之处。TF 仅衡量词的出现频次，
但是没有考虑到词的对文档的区分能力。

- TF 的计算常用表达式:

   :math:`{tf}_{ij}=\frac{n_{ij}}{\sum_{k} n_{kj}}` 

   :math:`tf(word) = \frac{word在文档中出现的次数}{文档总词数}` 

   - 其中：

      - :math:`n_{ij}` 表示词 :math:`i` 在文档 :math:`j` 中的出现频次
         
         - 但是仅用频次来表示，长文本中的词出现频次高的概率会更大，这一点会影响到不同文档之间关键词权值的比较，所以在计算的过程中一般还会对词频进行归一化

      - :math:`\sum_{k} n_{kj}` 是统计文档中每个词出现次数的总和，也就是文档的总词数

- IDF 的计算常用表达式:

   :math:`{idf}_i=log\Big(\frac{|D|}{1+|D_{i}|}\Big)` 

   - 其中

      - :math:`|D|` 为文档集中的总文档数

      - :math:`|D_{i}|` 为文档集中出现词 :math:`i` 的文档数量。分母加 1 是采用了拉普拉斯平滑，
        避免有部分新的词没有在语料库中出现过而导致分母为零的情况出现，增强算法的健壮性

TF-IDF 算法就是 TF 算法与 IDF 算法的综合使用，具体的计算方法如下：

   - :math:`tf \times idf(i, j) = {tf}_{ij} \times {idf}_{i} = \frac{n_{ij}}{\sum_{k} n_{kj}} \times log\Big(\frac{|D|}{1+|D_{i}|}\Big)` 


.. note:: 

   TF-IDF 算法也有很多变种的加权方法。传统的 TF-IDF 算法中，仅考虑了词的两个统计信息(出现频次、在多少个文档出现)，
   因此，其对文本的信息利用程度显然也是很少的。

   除了上面的信息外，在一个文本中还有许多信息能够对关键词的提取起到很好的知道作用，例如每个词的词性、出现的位置等。

   在某些特定的场景中，如在传统的 TF-IDF 基础上，加上这些辅助信息，能对关键词提取的效果起到很好的提高作用。

5.3 关键词提取算法--TextRank 算法
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TextRank 算法的基本思想来源于 Google 的 PageRank 算法。PageRank 算法是 Google 创始人拉里·佩奇和谢尔盖·布林与 1997 年构建早期的
的搜索系统原型时提出的链接分析算法，该算法是他们用来评价搜索系统过覆盖网页重要性的一种重要方法，随着 Google 的成功，该算法也成为其他搜索
引擎和学术界十分关注的计算模型。

PageRank 算法是一种网页排名算法，其基本思想有两条:

   - (1)链接数量

   - (2)链接质量

      - 一个网页被一个越高权值的网页链接，也能表明这个网页月重要






5.4 关键词提取算法--LSA, LSI, LDA 算法
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


5.5 关键词提取示例
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

5.5.1 关键词提取使用的 Python 库
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

   - ``jieba``

      - ``analyse`` 模块封装的 TextRank 算法

   - ``gensim``

      - Gensim 是一款开源的第三方 Python 工具包，用于从原始的非结构化文本中，无监督的学习到文本隐层的主题向量表达。
        它支持包括 TF-IDF、LSA、LDA 和 word2vec 在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，
        信息检索等一些常用任务的 API 接口

      .. code-block:: shell

         pip install genism

5.5.2 关键词提取算法步骤
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

训练一个关键词提取算法步骤：

   - 1.加载已有的文档数据集

   - 2.加载停用词表

   - 3.对数据集中的文档进行 **分词**

   - 4.根据停用词表，过滤干扰词

   - 5.根据数据集训练关键词提取算法

根据训练好的关键词提取算法对新文档进行关键词提取步骤：

   - 1.对新文档进行分词

   - 2.根据停用词表，过滤干扰词

   - 3.根据训练好的算法提取关键词

6.句法分析
----------------------

6.1 句法分析介绍
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

在自然语言处理中，机器翻译是一个重要的的课题，也是 NLP 应用的主要领域，而句法分析是机器翻译的核心数据结构。
句法分析是自然语言处理的核心技术，是对语言进行深层次理解的基石。

句法分析的主要任务是识别出句子所包含的 **句法成分** 以及这些 **成分之间的关系**，一般以 **句法树** 来表示
句法分析的结果。

句法分析一直是自然语言处理前进的巨大障碍，句法分析主要有以下两个难点：

   - **歧义**
      
      - 自然语言区别于人工语言的一个重要特点就是它存在大量的歧义现象。人类自身可以依靠大量的先验知识有效地消除各种歧义，
        而机器由于在知识表示和获取方面存在严重不足，很难像人类那样进行句法消歧。
      
   - **搜索空间**

      - 句法分析是一个极为复杂的任务，候选树个数随句子增多呈指数级增长，搜索空间巨大。因此，必须设计出合适的解码器，以确保能能够在
        可以容忍的时间内搜索到模型定义最优解。

**句法分析(Parsing)** 是从单词串得到句法结构的过程，而实现该过程的工具或程序被称为 **句法分析器(Parser)**。句法分析的种类很多，
根据其侧重目标将其分为:

   - **完全句法分析**

   - **局部句法分析**


句法分析中所用方法可以简单地分为两大类：

   - **基于规则的方法**

      - 基于规则的方法在处理大规模真实文本时，会存在语法规则覆盖有限、系统可迁移等缺陷
   
   - **基于统计的方法**

      - 随着大规模标注树库的建立，句法分析器的性能不断提高，最经典的就是 PCFG(Probabilistic Context Free Grammar)，
        它在句法分析领域得到了极大的应用，也是现在句法分析中常用的方法。

      - 统计句法分析模型本质上是一套面向候选树对的评价方法，其会给正确的句法树赋予一个较高的分值，而给不合理的句法树赋予一个较低的分值
        这样就可以借用候选句法树的分值进行消歧。


6.2 句法分析的数据集与评测方法
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

统计分析方法一般都离不开语料数据集和相应的评价体系的支撑。

6.2.1 句法分析的数据集
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



6.2.2 句法分析的评测方法
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^





6.3 句法分析的常用方法
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~












7.自然语言处理与词向量
----------------------

   自然语言处理主要研究使用计算机来处理、理解以及运用人类语言的各种理论和方法,属于人工智能的一个重要研究方向；


7.1 词汇表征
~~~~~~~~~~~~


7.2 词向量与语言模型
~~~~~~~~~~~~~~~~~~~~


8.word2vec 词向量
-----------------------------

   从深度学习的角度看,假设将 NLP 的语言模型看作是一个监督学习问题：给定上下文词 :math:`X`,输出中间词 :math:`Y`；
   或者给定中间词 :math:`X`,输出上下文词 :math:`Y`.基于输入 :math:`X` 和输出 :math:`Y` 之间的映射便是语言模型.
   这样的一个语言模型的目的便是检查 :math:`X` 和 :math:`Y` 放在一起是否符合自然语言规则,更通俗一点就是 :math:`X` 和
   :math:`Y` 放在一起是不是人话.

   所以,基于监督学习的思想,word2vec 便是一种基于神经网络训练的自然语言模型.word2vec 是谷歌于 2013 年提出的一种 NLP
   工具,其特点就是将词汇进行向量化,这样就可以定量的分析和挖掘词汇之间的联系.因而 word2vec 也是词嵌入表征的一种,
   只不过这种向量表征需要经过神经网络训练得到.

   word2vec 训练神经网路得到的一个关于输入 :math:`X` 和输出 :math:`Y` 之间的语言模型,关注的重点并不是说要把这个模型训练的有多好,
   而是要获取训练好的神经网络权重,这个权重就是我们要拿来对输入词汇 :math:`X` 的向量化表示.一旦拿到了训练预料所有词汇的词向量,接下来开展
   NLP 分析工作就相对容易一些.


9.词向量的训练
--------------------

