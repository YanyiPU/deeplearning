.. _header-n0:

DNN
========

.. _header-n3:

1. 机器学习与深度学习
---------------------

**机器学习** 就是从历史数据中探索和训练出数据的普遍规律, 将其归纳为相应的数学模型, 并对未知的数据进行预测的过程; 
在这个过程中会碰到各种各样的问题, 比如如下等一系列关乎机器学习模型生死的问题：

    - 数据质量
    
    - 模型评价标准
    
    - 训练优化方法
    
    - 过拟合

在机器学习中, 有很多已经相当成熟的模型, 在这些机器学习模型中, **人工神经网络** 就是一种比较厉害的模型; 
人工神经网络从早期的感知机发展而来, 对任何函数都有较好的拟合性.

但自上个世纪 90 年代一直到 2012 年深度学习集中爆发前夕, 神经网络受制于计算资源的限制和较差的可解释性, 一直处于发展的低谷阶段. 
之后大数据兴起,计算资源也迅速跟上, 加之 2012 年 ImageNet 竞赛冠军采用的 AlexNet 卷积神经网络一举将图片预测的 top5 错误率降至 16.4%, 
震惊了当时的学界和业界. 从此之后, 原本处于研究边缘状态的神经网络又迅速热了起来, 深度学习也逐渐占据了计算机视觉的主导地位.

以神经网络为核心的深度学习理论是机器学习的一个领域分支, 所以深度学习其本质上也必须遵循一些机器学习的基本要以和法则.

.. _header-n8:

2.感知机
--------

.. _header-n9:

2.1 感知机模型介绍
~~~~~~~~~~~~~~~~~~

**感知机, perceptron** 是由美国学者 Frank Rosenblatt 在 1957 年提出来的, 感知机是神经网络(深度学习)的起源算法. 
因此, 学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想, 感知机是神经网络的理论基础. 

感知机就是一个通过建立一个线性超平面, 对线性可分的数据集进行分类的线性模型. 感知机接收多个输入信号, 输出一个信号.

假设 :math:`x_1, x_2, \cdots, x_p` 是输入信号, :math:`\hat{y}` 是输出信号,
:math:`w_1,w_2,\cdots, w_p` 是权重, :math:`b` 是偏置.输入信号被送往神经元时,
会被分别乘以固定的权重 :math:`(w_1x_1,w_2x_2,\cdots,w_px_p)`.
神经元会计算传送过来的信号的总和,只有当这个总和超过了某个界限值时,才会输出 1.
这也被称为"神经元被激活”.这里将这个界限值称为阈值,用符号 :math:`\theta` 表示.

感知机的多个输入信号都有各自固定的权重,这些权重发挥着重要控制各个信号的重要性的作用.
也就是说,权重越大,对应该权重的信号的重要性就越高:

        :math:`\hat{y}=\sigma\Big(\sum_{i=1}^{p} \omega_i x_i + b\Big)`

        :math:`\begin{cases} 
        y = 1, \text{if} (\sum_{i=1}^{p} \omega_{i} x_{i} + b) > \theta \\
        y = 0, \text{if} (\sum_{i=1}^{p} \omega_{i} x_{i} + b) \leq \theta 
        \end{cases}`

感知机训练:

-  首先,模型接受输入 :math:`x_{i}`\ ,将输入 :math:`x_{i}`
   与权重(weights) :math:`\omega_i` 和偏置(bias) :math:`b` 进行加权求和
   :math:`\sum_{i=1}^{p} \omega_i x_i + b`\ ,并经过
   :math:`\sigma(\cdot)` 函数进行激活,将激活结果作为 :math:`\hat{y}`
   进行输出.这便是感知机执行前向传播计算的基本过程;

-  其次,当执行完前向传播计算得到输出 :math:`\hat{y}`
   之后,模型需要根据输出 :math:`\hat{y}` 和实际的样本标签(sample label)
   :math:`y` 按照损失函数 :math:`L(y, \hat{y})` 计算当前损失;

-  最后,通过计算损失函数 :math:`L(y, \hat{y})` 关于权重(weights)
   :math:`\omega_i` 和偏置(bias) :math:`b`
   的梯度,根据梯度下降算法更新权重和偏置.经过不断的迭代调整权重和偏置使得损失最小,这便是完整的
   **单层感知机** 的训练过程.

.. _header-n22:

2.2 感知机的局限性
~~~~~~~~~~~~~~~~~~

感知机的局限性就在于它只能表示由一条直线(超平面)分割的空间,即不能对非线性空间进行分割.

**异或门(XOR gate):**

-  感知机无法实现异或门.

    +-------------------+-------------------+-----------------+
    | 输入\ :math:`x_1` | 输入\ :math:`x_2` | 输出\ :math:`y` |
    +===================+===================+=================+
    | 0                 | 0                 | 0               |
    +-------------------+-------------------+-----------------+
    | 1                 | 0                 | 1               |
    +-------------------+-------------------+-----------------+
    | 0                 | 1                 | 1               |
    +-------------------+-------------------+-----------------+
    | 1                 | 1                 | 0               |
    +-------------------+-------------------+-----------------+

XOR 函数("异或"逻辑)是两个二进制值 :math:`x_1` 和 :math:`x_2` 的运算, 如上. 
当这些二进制值中恰好有一个为 1 时, XOR 函数返回值为 1, 其余情况下为 0.

XOR 函数提供了想要学习的目标函数 :math:`y=f^{*}(x)`, 模型给出了一个函数 :math:`y=f(x;\theta)`, 
并且学习算法会不断调整参数 :math:`\theta` 来使得 :math:`f` 尽可能接近 :math:`f^{*}`.

假设不关心统计泛化,且把这个问题当做回归问题,使用均方误差损失函数.希望网络在4个点 :math:`X={[0,0]^{T}, [0,1]^{T}, [1,0]^{T},[1,1]^{T}}`
上表现正确.用全部这4个点来训练网络.

**多层感知机实现异或门:**

   -  感知机的局限性,严格来讲应该是:单层感知机无法表示异或门或者单层感知机无法分离非线性空间.

   -  感知机的绝妙之处在于它可以"叠加层”,通过叠加层可以表示异或门.

异或门可以通过组合与非门,或门,再将前两个逻辑门的组合和与门组合得到.\ :math:`x_1, x_2`
表示输入信号, :math:`y` 表示输出信号, :math:`x_1, x_2`
是与非门和或门的输入,而与非门和或门的输出则作与门的输入:

    .. code-block:: python

        def XOR(x1, x2):
            s1 = NAND(x1, x2)
            s2 = OR(x1, x2)
            y = AND(s1, s2)

            return y

异或门是一种多层结构的神经网络, 这里将与非门, 或门称为第0层, 与门称为第1层, 异或门的输出称为第3层.
实际上与门, 或门, 与非门是单层感知机, 而异或门是多层感知机. 叠加了多层的感知机也称为 **多层感知机(multi-layered perceptron)**.

.. _header-n64:

2.3 从感知机到神经网络
~~~~~~~~~~~~~~~~~~~~~~

- **单层感知机** 包含两层神经元,即输入与输出神经元,可以非常容易的实现逻辑与、或和非等线性可分情形, 
  但终归而言,这样的一层感知机的学习能力是非常有限的, 对于像异或这样的非线性情形, 单层感知机就搞不定了.
  其学习过程会呈现一定程度的振荡,权值参数 :math:`\omega_i` 难以稳定下来,最终不能求得合适的解.

- 对于 **非线性可分** 的情况, 在感知机的基础上一般有了两个解决方向:

   - **支持向量机模型**: 旨在通过 **核函数** 映射来处理非线性的情况.

   - **神经网络模型**: 神经网络模型也叫 **多层感知机(MLP: Muti-Layer Perception)**, 与单层的感知机
     在结构上的区别主要在于 MLP 多了若干 **隐藏层**, 这使得神经网络对非线性的情况拟合能力大大增强.

.. _header-n75:

3. 神经网络
-----------

.. _header-n76:

3.1 深度前馈网络
~~~~~~~~~~~~~~~~

.. _header-n77:

3.1.1 概念
^^^^^^^^^^

**神经网络分类:**

-  前馈神经网络(Feedforward neural network)

   -  深度前馈网络(deep feedforward network)

   -  多层感知机(Multilayer perceptron, MLP)

-  反馈神经网络(FeedBack neural network)

   -  循环神经网络(recurrent neural network)

**神经网络结构:**

-  深度

-  宽度

-  第一层,第二层,...

-  隐藏层

-  输出层

**深度前馈网络介绍:**

深度前馈网络的目标是:近似某个函数 :math:`f^{*}`, 例如,对于分类器
:math:`y=f^{*}(x)`\ ,将输入 :math:`x` 映射到一个类别 :math:`y`.
深度前馈网络定义了一个映射 :math:`y = f(x; \theta)`\ , 并且学习参数
:math:`\theta` 的值,使它能够得到最佳的函数近似.
在神经网络训练过程中,让 :math:`f(x)` 去匹配 :math:`f^{*}(x)`
的值,训练数据为我们提供了在不同训练数据点上取值的、含有噪声的
:math:`f^{*}(x)` 的近似实例,每个样本 :math:`x` 都伴随着一个标签
:math:`y \approx f^{*}(x)`\ . 训练数据直接指明了输出层在每一个
:math:`x`\ 上必须做什么;它必须产生一个接近 :math:`y`
的值.但是训练数据并没有直接指明其它层应该怎么做.学习算法必须决定如何使用这些层来产生想要的输出,但是训练数据并没有说每个单独的层应该做什么.相反,学习算法必须决定如何使用这些层来最好地实现
:math:`f^{*}`\ 的近似.因为训练数据并没有给出这些层中每一层所需的输出,所以这些层被称为隐藏层.

-  深度前馈网络之所以被称为 **前馈(feedforward)** 的,是因为信息流经过过
   :math:`x` 的函数,流经用于定义 :math:`f` 的中间计算过程,最终到达输出
   :math:`y`\ .在模型的输出和模型本身之间没有反馈(feedback)连接.当深度前馈网络被扩展成包含反馈连接时,被称为循环神经网络(recurrent
   neural network).

-  深度前馈网络之所以被称为
   **网络(network)**\ ,是因为它们通常用许多不同函数复合在一起来表示.该模型与一个有向无环图相关联,而图描述了函数是如何复合在一起的.

-  深度前馈网络之所以被称为
   **神经网路**\ ,是因为他们或多或少地受到神经科学的启发.网络中每个隐藏层通常都是向量值的.这些隐藏层的维数决定了模型的宽度(width).向量的每个元素都可以被视为起到类似一个神经元的作用.除了将层想象成向量到向量的单个函数,也可以把层想象成由许多并行操作单元(unit)组成,每个单元表示一个向量到标量的函数.每个单元在某种意义上类似一个神经元,它接收的输入来源于许多其他的单元,并计算自己的激活值.

**深度前馈网络设计:**

-  选择优化模型、代价函数、输出单元形式

-  选择用于计算隐藏层值激活函数(activation function)

-  设计网络的结构,包括网络应该包含多少层,层与层之间应该如何连接,以及每一层包含多少单元

-  反向传播(back propagation)算法和推广

.. _header-n124:

3.1.2 线性模型的局限性及克服
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**线性模型的局限性:**

线性模型,如逻辑回归和线性回归, 是非常吸引人的, 因为无论是通过闭解形式还是使用凸优化, 它们都能高效且可靠地拟合.
线性模型也有明显的缺陷: 模型的能力被局限在线性函数里, 所以无法理解任何两个输入变量之间的相互作用.

**克服线性模型的局限性:**

为了扩展线性模型来表示 :math:`x` 的非线性函数,可以不把线性模型用于
:math:`x` 本身,而是用在一个变换后的输入 :math:`\phi(x)` 上,这里
:math:`\phi` 是一个非线性学习算法,可以认为 :math:`\phi` 提供了一组描述
:math:`x` 的特征,或者认为它提供了 :math:`x` 的一个新的表示.

如何选择映射 :math:`\phi`\ ?

1. 其中一种选择是使用一个通用的 :math:`\phi`\ ,例如无限维的
   :math:`\phi`\ ,它隐含地用在基于 RBF 核的核机器上.

2. 另一种选择是手动设计 :math:`\phi`\ ,传统的机器学习模型.

3. 深度学习的策略是去学习 :math:`\phi`\ .在这种方法中,有一个模型
   :math:`y=f(x;\theta,\omega)= \phi(x;\theta)^{T}\omega`\ ,现在有两种参数:用于从一大类函数中学习
   :math:`\phi` 的参数 :math:`\theta`\ ,以及用于将 :math:`\phi(x)`
   映射到所需的输出的参数 :math:`\omega`\ .其中 :math:`\phi`
   定义了一个隐藏层.即:通过学习特征来改善模型.

.. _header-n138:

3.2 隐藏层的设计
~~~~~~~~~~~~~~~~

.. _header-n139:

3.2.1 隐藏层激活函数
^^^^^^^^^^^^^^^^^^^^

   -  感知机中使用了阶跃函数作为激活函数,阶跃函数以阈值为界,一旦输入超过阈值,就切换输出;

   -  如果感知机使用其他函数 (sigmoid, ReLU) 作为激活函数,就可以进入神经网络的世界了;

**阶跃函数:**

感知机中使用了阶跃函数作为激活函数,阶跃函数以阈值为界,一旦输入超过阈值,就切换输出

   :math:`h(x)=\left\{
   \begin{array}{rcl} 
   0 & & {x \leq 0} \\ 
   1 & & {x > 0}    \\
   \end{array} \right.`


**sigmoid函数:**

神经网络中用 sigmoid 函数作为激活函数,进行信号的转换,转换后的信号被传送给下一个神经元

   :math:`h(x) = \frac{1}{1+e^{-x}}, 其中:e是纳皮尔常数 2.7182...`


**ReLU(Rectified Linear Unit, 整流线性单元)函数:**

在神经网络发展的历史上,sigmoid 函数很早就开始使用了,而最近则主要使用 **ReLU(Rectified Linear Unit)** 函数

   :math:`h(x)=\left\{
   \begin{array}{rcl}
   x    &      & {x > 0}    \\
   0    &      & {x \leq 0} \\
   \end{array} \right.`

**阶跃函数、sigmoid激活函数、ReLU激活函数实现及比较:**

阶跃函数的实现:

.. code:: python

   # 简单函数形式,只接受浮点数为参数
   def step_function(x):
      if x > 0:
         return 1
      else:
         return 0

   # 支持Numpy数组的实现
   def setp_function(x):
      y = x > 0
      return y.astype(np.int)

阶跃函数的图形:

.. code:: python

   import numpy as np
   import matplotlib.pylab as plt

   x = np.arange(-5.0, 5.0, 0.1)
   y = step_function(x)
   plt.plot(x, y)
   plt.ylim(-0.1, 1.1)
   plt.show()

sigmoid激活函数实现:

.. code:: python

   def sigmoid(x):
      y = 1 / (1 + np.exp(x))
      return y

sigmoid函数的图形:

.. code:: python

   import numpy as np
   import matplotlib.pylab as plt

   x = np.arange(-5.0, 5.0, 0.1)
   y = sigmoid(x)
   plt.plot(x, y)
   plt.ylim(-0.1, 1.1)
   plt.show()

ReLU函数实现:

.. code:: python

   def relu(x):
      y = np.maximum(0, x)

ReLU函数的图形:

.. code:: python

   import numpy as np
   import matplotlib.pylab as plt

   x = np.arange(-5.0, 5.0, 0.1)
   y = relu(x)
   plt.plot(x, y)
   plt.ylim(-0.1, 1.1)
   plt.show()

激活函数比较:

阶跃函数与Sigmoid函数:

-  Sigmoid
   函数是一条平滑的曲线,输出随着输入发生连续性的变化;而阶跃函数以 0
   为界,输出发生急剧性的变化,Sigmoid
   函数的平滑性对神经网络的学习具有重要意义;

-  相对于阶跃函数只能返回 0 或 1,sigmoid
   函数可以返回实数,也就是说,感知机中神经元之间流动的是0或1的二元信号,而神经网络中流动的是连续的实数值信号;

-  阶跃函数和 Sigmoid 函数的结构均是"输入小时,输出接近0(0),
   随着输入增大,输出向1靠近(1)”,也就是说,当输入信号为重要信息时,两个函数都会输入较大的值;当输入信号不重要的信号时,两者都输入较小的值;但不管信号大小,输出信号的值都在0到1之间;

-  阶跃函数和Sigmoid函数均为\ **非线性函数**\ .神经网络的激活函数必须使用非线性函数,激活函数不能使用线性函数,因为使用线性函数的话,加深神经网络的层数就没有意义了;

阶跃函数,Sigmoid函数,ReLU函数:

.. _header-n181:

3.3 输出层的设计
~~~~~~~~~~~~~~~~

   -  神经网络可以用在分类和回归问题上,不过需要根据情况改变输出层的激活函数;

   -  一般而言,回归问题用\ ``恒等函数``\ ,分类问题用\ ``softmax``\ 函数;

.. _header-n188:

3.3.1 输出层激活函数
^^^^^^^^^^^^^^^^^^^^

**恒等函数:**

-  恒等函数的形式

:math:`\sigma(x) = x`

.. code:: python

   def identity_function(x):
      return x

**softmax函数:**

-  softmax函数的形式

:math:`y_k = \frac{e^{a_{k}}}{\sum_{i=1}^{n}e^{a_i}}`

其中:

-  :math:`n`\ 是输出层神经元的个数

-  :math:`k`\ 是指第\ :math:`k`\ 个神经元

-  :math:`a是输入信号`

.. code:: python

   def softmax(a):
      exp_a = np.exp(a)
      sum_exp_a = np.sum(exp_a)
      y = exp_a / sum_exp_a
      
      return y

-  softmax函数针对\ ``溢出``\ 问题的改进

:math:`y_k = \frac{e^{a_k+C}}{\sum_{n}^{i=1}e^{a_i+C}}`

.. code:: python

   def softmax(a):
      c = np.max(a)
      exp_a = np.exp(a - c)
      sum_exp_a = np.sum(np.exp(a - c))
      y = exp_a / sum_exp_a

      return y

.. _header-n215:

3.3.2 输出层的神经元数量
^^^^^^^^^^^^^^^^^^^^^^^^

-  输出层的神经元数量需要根据待解决的问题决定;

-  对于分类问题,输出层的神经元数量一般设定为类别的数量;

.. _header-n222:

3.5.2 批处理
^^^^^^^^^^^^

-  ``批(batch)处理``\ 推理流程抽象

   -  批处理对计算机的运算大有利处,可以大幅缩短每张图像的处理时间

   -  大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化,并且在神经网络运算中,当数据传送成为瓶颈时,批处理可以减轻数据总线的负荷,也就是说,批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快.
