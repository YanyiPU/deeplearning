.. _header-n0:

RNN-LSTM
========

.. _header-n3:

1. LSTM
-------

.. _header-n4:

1.1 梯度爆炸与梯度消失
~~~~~~~~~~~~~~~~~~~~~~

在普通深度神经网络和深度卷积网络中，当网络结构变深时，神经网络在训练时会碰到梯度爆炸或者梯度消失的情况。那么什么是梯度爆炸和梯度消失呢？它们又是怎样产生的呢？

不管是哪种类型的神经网络，其训练都是通过反向传播计算梯度来实现权重更新的。我们通过设定损失函数，建立损失函数关于各层网络输入输出的梯度计算，当网络训练开动起来的时候，系统便按照反向传播机制来不断更新网络各层参数直到停止训练。但当网络层数加深时，这个训练系统并不是很稳，经常会出现一些问题。其中梯度爆炸和梯度消失便是最大的问题之二。

-  梯度爆炸

   -  在神经网络训练过程中，梯度变得越来越大，使得神经网络权重得到疯狂更新的情形，这种情况很容易发现，因为梯度过大，计算更新得到的参数也会大到崩溃，这时候我们可能看到更新的参数值中有很多的
      NaN，这说明梯度爆炸已经使得参数更新出现数值溢出

-  梯度消失

   -  与梯度爆炸相反的是，梯度消失就是在神经网络训练过程中梯度变得越来越小以至于梯度得不到更新的一种情形。当网络加深时，网络深处的误差因为梯度的减小很难影响到前层网络的权重更新，一旦权重得不到有效的更新计算，神经网络的训练机制也就失效了

神经网络训练过程中梯度怎么就会变得越来越大或者越来越小呢？以神经网络反向传播推导公式为例来解释：

上述公式是一个两层网络的反向传播参数更新公式推导过程。离输出层相对较远的是输入到隐藏层的权重参数，可以看到损失函数对于隐藏层输出
a1输入到隐藏层权重W1和偏置b1的梯度计算公式，一般而言都会转换从下一层的权重乘以激活函数求导后的式子。如果激活函数求导后的结果和下一层权重的乘积大于1或者说远远大于1的话，在网络层数加深时，层层递增的网络在做梯度更新时往往就会出现梯度爆炸的情况。如果激活函数求导和下一层权重的乘积小于1的话，在网络加深时，浅层的网络梯度计算结果会越来越小往往就会出现梯度消失的情况。所以可是说是反向传播的机制本身造就梯度爆炸和梯度消失这两种不稳定因素。比如说一个
100
层的深度神经网络，假设每一层的梯度计算值都为1.1，经过由输出到输入的反向传播梯度计算可能最后的梯度值就变成
1.1^100 =
13780.61234，这是一个极大的梯度值了，足以造成计算溢出问题。若是每一层的梯度计算值为
0.9，反向传播输入层的梯度计算值则可能为 0.9^100 =
0.000026561398，足够小到造成梯度消失。（例子只是一个简化的假设情况，实际反向传播计算要更为复杂。）

所以总体来说，神经网络的训练中梯度过大或者过小引起的参数过大过小都会导致神经网络失效，那我们的目的就是要让梯度计算回归到正常的区间范围，不要过大也不要过小，这也是解决这两个问题的一个思路。下图是一个
4 层网络的训练过程各参数的取值范围：

那么梯度爆炸和梯度消失怎么解决呢？梯度爆炸并不麻烦，在实际训练的时候对梯度进行修剪即可，但是梯度消失的处理就比较麻烦了，由上述的分析我们知道梯度消失一个关键在于激活函数。sigmoid
激活函数本身就更容易产生这种幺蛾子，所以一般而言，我们换上更加鲁棒的
ReLu
激活函数以及给神经网络加上归一化激活函数层，一般问题都能得到很好的解决，但也不是任何情形下都管用，比如说咱们的
RNN 网络，具体在下文中我们在做集中探讨。

.. _header-n24:

1.2 LSTM：让RNN具备更好的记忆机制
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   -  梯度爆炸和梯度消失对RNN的影响非常大，当RNN加深时，因为梯度消失的问题使得前层的网络权重得不到更新，RNN的记忆性就很难生效；

   -  在传统的RNN基础上，研究人员给出了一些著名的改进方案，即RNN变种网络，比较著名的是GRU(循环门控单元)和LSTM(长短期记忆网络)；GRU和LSTM二者的结构基本一致，但有些许不同；

   -  LSTM

      -  LSTM的本质是一种RNN网络；

      -  LSTM在传统的RNN结构上做了相对复杂的改进，这些改进使得LSTM相对于经典的RNN能够很好地解决梯度爆炸和梯度消失的问题，让RNN具备更好的记忆性能；

.. _header-n40:

2.2.1 LSTM结构
^^^^^^^^^^^^^^

**经典RNN结构与LSTM结构对比：**

   相对经典RNN，LSTM单元中包含了4个交互的网络层；

**LSTM结构数学表示：**

forget-gate:

:math:`\Gamma_{f}^{(t)} = \sigma_f (W_f [a^{(t-1)}, x^{(t)}] + b_{f})`

update-gate:

:math:`\Gamma_{u}^{(t)} = \sigma_u (W_u [a^{(t-1)}, x^{(t)}] + b_{u})`

:math:`\widetilde{c}^{(t)} = \tanh (W_c [a^{(t-1)}, x^{(t)}] + b_{c})`

remember-cell update:

:math:`c^{(t)} = \Gamma_{f}^{(t)} \circ c^{(t-1)} + \Gamma_{u}^{(t)} \circ \widetilde{c}^{(t)}`

output-gate:

:math:`\Gamma_{o}^{(t)} = \sigma_o (W_o [a^{(t-1)}, x^{(t)}] + b_{o})`

:math:`a^{(t)} = \Gamma_{o}^{(t)} \circ \tanh(c^{(t)})`

Output:

:math:`y^{(t)} = softmax(a^{(t)})`

**LSTM结构分解：**

-  记忆细胞(remember cell)

   -  在LSTM单元的最上层有一条贯穿的关于记忆细胞 :math:`c^{(t-1)}` 到
      :math:`c^{(t)}`
      的箭头直线；这样贯穿的直线表现记忆信息在网络各层之间保持下去很容易

   -  记忆细胞表示：

:math:`c^{(t-1)} \rightarrow c^{(t)}`

-  遗忘门(forget gate)

   -  所谓的遗忘门就是要决定从记忆细胞中是否丢弃某些信息；通过一个sigmoid函数处理；

   -  遗忘门接受来自输入 :math:`x^{(t)}` 和上一层隐状态
      :math:`a^{(t-1)}` 的值进行加权计算处理；

   -  遗忘门计算公式：

:math:`\Gamma_{f}^{(t)} = \sigma_f (W_f [a^{(t-1)}, x^{(t)}] + b_{f})`

-  更新门(update gate)

   -  更新们就是需要确定什么样的信息能够存入细胞状态中，这跟GRU中类似，除了计算更新们之外，还需要通过\ :math:`tanh`
      计算记忆细胞的候选值 :math:`\widetilde{c^{(t)}}`\ ；

   -  更新门计算公式：

:math:`\Gamma_{u}^{(t)} = \sigma_u (W_u [a^{(t-1)}, x^{(t)}] + b_{u})`

:math:`\widetilde{c}^{(t)} = \tanh (W_c [a^{(t-1)}, x^{(t)}] + b_{c})`

LSTM结合遗忘门、更新门、上一层的记忆细胞和记忆细胞候选值来共同决定和更新当前细胞状态：

:math:`c^{(t)} = \Gamma_{f}^{(t)} \circ c^{(t-1)} + \Gamma_{u}^{(t)} \circ \widetilde{c}^{(t)}`

-  输出门(output)

   -  LSTM提供了单独的输出门；

   -  输出门计算公式：

:math:`\Gamma_{o}^{(t)} = \sigma_o (W_o [a^{(t-1)}, x^{(t)}] + b_{o})`

:math:`a^{(t)} = \Gamma_{o}^{(t)} \circ \tanh(c^{(t)})`
