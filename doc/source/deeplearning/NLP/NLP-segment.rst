
NLP--分词
=================

2.中文分词(segment)
--------------------------------------

   - 在语言理解中, 词是最小的能够独立活动的有意义的语言成分. 将词确定下来是理解自然语言的第一步, 只有跨越了这一步, 
     中文才能像英文那样过度到短语划分、概念抽取以及主题分析, 以致自然语言理解, 最终达到智能计算的最高境界.

   - “词” 的概念一直是汉语言语言学界纠缠不清而又绕不开的问题. 主要难点在于汉语结构与印欧体系语种差异甚大, 
     对词的构成边界方面很难进行界定. 比如, 在英语中, 单词本身就是 “词” 的表达, 一篇英文文章就是 “单词” 加分隔符(空格)来表示的.

   - 在汉语中, 词以字为基本单位的, 但是一篇文章的语义表达却仍然是以词来划分的. 因此, 在处理中文文本时, 需要进行分词处理, 
     将句子转化为词的表示. 这个切词处理过程就是 **中文分词**, 它通过计算机自动识别出句子的词, 在词间加入边界标记符, 分隔出各个词汇. 
     整个过程看似简单, 然而实践起来却很复杂, 主要的困难在于 **分词歧义**. 其他影响分词的因素是: **未登录词**、**分词粒度粗细**.

   - 自中文自动分词被提出以来, 历经将近 30 年的探索, 提出了很多方法, 可主要归纳为：

      - 规则分词

      - 统计分词

      - 混合分词(规则+统计)

2.1 规则分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   基于规则的分词是一种机械的分词方法,主要通过维护词典,在切分语句时,将语句的每个字符串与词汇表中的词逐一进行匹配,
   找到则切分,否则不予切分.

   按照匹配切分的方式,主要有 **正向最大匹配法**、**逆向最大匹配法** 以及 **双向最大匹配法**.

2.1.1 正向最大匹配法
^^^^^^^^^^^^^^^^^^^^^

   .. code-block:: python

      class MM(object):
         def __init__(self):
            self.window_size = 3

         def cut(self, text):
            result = []
            index = 0
            text_length = len(text)
            dic = ["研究", "研究生", "生命", "命", "的", "起源"]
            while text_length > index:
                  for size in range(self.window_size + index, index, -1):
                     piece = text[index:size]
                     if piece in dic:
                        index = size - 1
                        break
                  index = index + 1
                  result.append(piece + "----")
            print(result)

      if __name__ == "__main__":
         text = "研究生命的起源"
         tokenizer = MM()
         print(tokenizer.cut(text))


2.1.2 反向最大匹配法
^^^^^^^^^^^^^^^^^^^^^


2.1.3 双向最大匹配法
^^^^^^^^^^^^^^^^^^^^^




2.2 统计分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   统计分词的主要思想是：把每个词看做是由词的最小单位的各个字组成的,如果相连的字在不同的文本中出现的次数越多,就证明这相连的字很可能就是一个词.
   因此我们就可以利用 **字与字相邻出现的频率** 来反应 **成词的可靠度**,统计语料中相邻共现的各个字的组合的频度,当组合频度高于某一个临界值时,便可以认为此字组
   成会构成一个词语.

   基于统计的分词,一般要做如下两步操作：

2.3 混合分词
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



-  常用分词库

   -  StanfordNLP

   -  哈工大语言云

   -  庖丁解牛分词

   -  盘古分词 (ICTCLAS, 中科院汉语词法分析系统)

   -  IKAnalyzer（Luence项目下，基于java）

   -  FudanNLP（复旦大学）

   -  中文分词工具

   -  ``Ansj``

   -  盘古分词

   -  ``jieba``

1.jieba 分词
---------------------

1.1 安装
~~~~~~~~~~~~~~~~~~~~~

    .. code-block:: shell

        $ pip install paddlepaddle-tiny=1.6.1 # Python3.7
        $ pip install jieba

1.2 特点、算法
~~~~~~~~~~~~~~~~~~~~~

    - 特点:

        - 支持四种分词模式：

            - 精确模式，试图将句子最精确地切开，适合文本分析

            - 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义

            - 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词

            - paddle 模式，利用 PaddlePaddle 深度学习框架，训练序列标注(双向GRU)网络模型实现分词。同时支持词性标注

                - paddle 模式使用需安装 ``paddlepaddle-tiny``

                - 目前 paddle 模式支持 jieba v0.40 及以上版本

        - 支持繁体分词

        - 支持自定义词典

        - MIT 授权协议

    - 算法:

        - 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)

        - 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合

        - 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

1.4 主要功能
~~~~~~~~~~~~~~~~~~~~~

1.4.1 分词 API
^^^^^^^^^^^^^^^^^^^^^

    - ``jieba.enable_paddle()``

    - ``jieba.cut(sentence = "", cut_all = False, HMM = True, use_paddle = False)``

    - ``jieba.lcut(sentence = "", cut_all = False, HMM = True, use_paddle = False)``

    - ``jieba.cut_for_search(sentence = "", HMM = True)``

    - ``jieba.lcut_for_search(sentence = "", HMM = True)``

1.4.2 添加自定义词典
^^^^^^^^^^^^^^^^^^^^^

    - jieba.load_userdict(file_name): 载入自定义词典

    - jieba.dt.tmp_dir
    
    - jieba.dt.cache_file

    - add_word(word, freq = None, tag = None)

    - del_word(word)

    - suggest_freq(segment, tune - True)

1.4.3 关键词提取
^^^^^^^^^^^^^^^^^^^^^





1.4.4 词性标注
^^^^^^^^^^^^^^^^^^^^^






1.4.5 并行分词
^^^^^^^^^^^^^^^^^^^^






1.4.6 Tokenize：返回词语在原文的起止位置
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^




1.4.7 ChineseAnalyzer for Whoosh 搜索引擎
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



1.4.8 命令行分词
^^^^^^^^^^^^^^^^^^^^











2.其他分词
----------------------


