
NLP--文本聚类
============================

1.无监督学习
----------------------------

   无监督学习希望能够发现数据本身的规律和模式，与监督学习相比，无监督学习不需要对数据进行标记。这样可以解决大量人力、物力，
   也可以让数据的获取变得非常容易。

   某种程度上说，机器学习的终极目标就是无监督学习。从功能上看，无监督学习可以帮助我们发现数据的“簇”，
   同时也可以帮助我们找寻“离群点(outlier)”。
   
   此外，对于特征维度特别高的数据样本，我们同样可以通过无监督学习对数据进行降维，
   保留数据的主要特征，这样对高维空间的数据也可以进行处理。

   常见的非监督学习任务：

      - 聚类
         - 需要定义相似性
      - 子空间估计
         - 通常研究如何将原始数据向量在更低维度下表示
         - 理想情况下，子空间的表示要具有代表性从而才能与原始数据接近，最常用的方法叫做主成分分析
      - 表征学习
         - 希望在欧几里得空间中找到原始对象的表示方式，从而能在欧几里得空间里表示出原始对象的符号性质
         - 例如：希望找到城市的向量表示，从而使得可以进行这样的向量运算：首都 + 美国 = 华盛顿
      - 生成对抗网络
         - 描述数据的生成过程，并检查真实数据与生成数据是否统计上相似

2.文本聚类
----------------------------

   聚类视图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇(cluser)”。
   通过这样的划分，每个簇可能对应于一些潜在的类别。这些概念对聚类算法而言事先是未知的，
   聚类过程仅能自动形成簇结构，簇所对应的含义需要由使用者来把握和命名。
   聚类常用于寻找数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。

   - 在 NLP 领域，一个很重要的应用方向是文本聚类，文本聚类有很多中算法，例如：K-means、DBScan、BIRCH、CURE 等。
   - 文本聚类存在大量的使用场景，比如数据挖掘、信息检索、主题检测、文本概括。
   - 聚类算法中初始的聚类点对后续的最终划分有非常大的影响，选择合适的初始点，可以加快算法的收敛速度和增强类之间的区分度。
     选择初始聚类点的方法有如下几种：

      - 随机选择法
         - 随机的选择 k 个对象作为初始聚类点
      - 最小最大法
         - 先选择所有对象中的相距最遥远的两个对象作为聚类点。然后选择第三个点，使得它与确定的聚类点的最小距离是所有点中最大的，
           然后按照相同的原则选取
      - 最小距离法
         - 选择一个正数 r，把所有对象的中心作为第一个聚类点，然后依次输入对象，当前输入对象与确认的聚点的距离都大于 r 时，
           则该对象作为新的聚类点
      - 最近归类法
         - 划分方法就是决定当前对象应该分到哪个簇中。
         - 划分方法中最为流行的是最近归类法，即将当前对象归类与最近的聚类点


