
NLP--特征工程
=======================

1.NLP 数据预处理
-----------------------------------------------

- 处理内容

    - 处理语料库文章

    - 处理语料库句子

- 处理工具

    - 正则表达式

        - re.match

        - re.search

- 处理方法

    - 语义解析器

    - 词频-逆向文件频率 (Term Frequency-Inverse Document Frequency, TF-IDF)

    - 词向量 word to vector(word2vec, Google-Tomas Mikolov, 2013)

    - 词嵌入 word embedding

2.NLP 特征工程
-----------------------------------------------

2.1 TF-IDF
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    - TF-IDF的主要思想是：如果包含词 t 的文档越少，也就是 n 越小，IDF 越大，则说明词 t 具有很好的类别区分能力​

    - TF-IDF 倾向于过滤掉常见的词语，保留重要的词语​

2.2 word2vec
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    - 词到向量

2.3 word embedding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



2.4 para2vec
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    - 段落到向量

2.5 doc2vec
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    - 文章到向量


2.6 GloVe
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    - 通过余弦函数、欧几里得距离来获得相似词的库

2.7 离散表示--One-hot
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

在一个语料库中，给每个字、词编码一个索引，根据索引进行 One-hot 表示。

- 文本语料

    - 如果只需要表示文本语料中的单词，可以只对其中出现过的单词进行索引编码即可

    .. code-block:: 
    
        // 语料
        John likes to watch moives. Mary likes too.
        John also likes to watch football games.

        // 编码
        {
            "John": 1, 
            "likes": 2,
            "to": 3,
            "watch": 4,
            "moives": 5,
            "also": 6,
            "football": 7,
            "games": 8,
            "Mary": 9,
            "too": 10
        }

- 文本 One-hot

    - 对其中每个单词用 One-hot 方法表示

    .. code-block:: 
    
        John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        likes: [1, 2, 0, 0, 0, 0, 0, 0, 0, 0]
        ...

.. note:: 

    - 文本 One-hot 的缺点
        
        - 当语料库非常大时，需要建立一个很大的字典对所有单词进行索引编码。
          比如 100W 个单词，每个单词就需要表示成 100W 维的向量，而且这
          个向量是很稀疏的，只有一个地方为 1 其他全为 0。还有很重要的一点，
          这种表示方法无法表达单词与单词之间的相似程度，如 beautiful 和 
          pretty 可以表达相似的意思但是 One-hot无法将之表示出来。​

2.8 Bag of Words
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2.9 Bi-gram, N-gram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2.10 Co-occurence Matrix
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


2.11 NNLM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2.12 CBOW
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2.13 Skip-gram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


2.14 Fasttext
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


